{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_similarity_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMRcxMNjQXzK",
        "outputId": "db1e6633-c268-4712-b5ef-3eb4ceea8931"
      },
      "source": [
        "!pip install mtcnn\n",
        "import mtcnn\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torchvision.transforms import ToTensor,Normalize,Compose,Scale\n",
        "from torch.nn import TripletMarginLoss\n",
        "from torch.nn import Module, Conv2d, ReLU, MaxPool2d, Linear, Dropout, BatchNorm2d\n",
        "from torch.nn import BatchNorm1d\n",
        "from torch.nn.init import kaiming_uniform_, xavier_uniform_\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torchsummary import summary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mtcnn in /usr/local/lib/python3.7/dist-packages (0.1.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn) (4.1.2.30)\n",
            "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn) (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python>=4.1.0->mtcnn) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->mtcnn) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->mtcnn) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->mtcnn) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras>=2.0.0->mtcnn) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIUWNuL2NGB6",
        "outputId": "87fb383e-72f0-4ebc-d649-18672e7324dd"
      },
      "source": [
        "#downlaoding the dataset\n",
        "!gdown --id 12_WTFi9ppvD-loaWUWpUar25Z3nT5k9P\n",
        "!unzip trainset.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12_WTFi9ppvD-loaWUWpUar25Z3nT5k9P\n",
            "To: /content/trainset.zip\n",
            "448MB [00:04, 93.5MB/s]\n",
            "Archive:  trainset.zip\n",
            "replace trainset/0001/0001_0000255/0000001.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U9FcvxiSlWW"
      },
      "source": [
        "#Loading the dataset in array format\n",
        "database =[]\n",
        "selfies =[]\n",
        "names =[]\n",
        "for filename in os.listdir('trainset'):\n",
        "  for subfiles in os.listdir(os.path.join('trainset',filename)):\n",
        "    db = []\n",
        "    data =[]\n",
        "    for subfile in os.listdir(os.path.join('trainset',filename,subfiles)):\n",
        "      if subfile.endswith('script.jpg'):\n",
        "        db.append(cv2.imread(os.path.join('trainset',filename,subfiles,subfile)))\n",
        "      else:\n",
        "        data.append(cv2.imread(os.path.join('trainset',filename,subfiles,subfile)))\n",
        "    database.append(db)\n",
        "    selfies.append(data)\n",
        "    names.append(subfiles)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWYk6G4oSmdj",
        "outputId": "867bc87e-323d-4062-dea8-2700a6863627"
      },
      "source": [
        "print('shape of selfies :',selfies[0][0].shape)\n",
        "print('shape of database photoes : ',database[0][0].shape)\n",
        "print(len(selfies),len(database))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of selfies : (325, 400, 3)\n",
            "shape of database photoes :  (275, 200, 3)\n",
            "1012 1012\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RphLpbkyWUzb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "82de60d9-5254-45f0-9c58-45edc89f63c3"
      },
      "source": [
        "# For face detection using MTCNN model \n",
        "'''\n",
        "face_detector = MTCNN()\n",
        "\n",
        "def face_detect(face):\n",
        "  result = face_detector.detect_faces(face)\n",
        "  if result :\n",
        "    x1,y1,w,h = result[0]['box']\n",
        "    x2,y2 = np.abs(x1)+w,np.abs(y1)+h\n",
        "    face = face[y1:y2,x1:x2]\n",
        "    return face\n",
        "  else:\n",
        "    return None \n",
        "\n",
        "\n",
        "trans = Compose([ToTensor(),Normalize(mean=0.4,std=0.2)])\n",
        "\n",
        "def prepare_data(data):\n",
        "  dataset=[]\n",
        "  count=0\n",
        "  for i in range(len(data)):\n",
        "    face=[]\n",
        "    for j in range(len(data[i])):\n",
        "      f= face_detect(data[i][j])\n",
        "      if f is not None:\n",
        "        if len(f)!=0:\n",
        "          f=cv2.resize(f,(240,240))\n",
        "          f= trans(f)\n",
        "          face.append(f)\n",
        "      else:\n",
        "        count+=1\n",
        "    if len(face)>0:\n",
        "      dataset.append(face)\n",
        "  print(count)\n",
        "  return dataset            \n",
        "'''  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nface_detector = MTCNN()\\n\\ndef face_detect(face):\\n  result = face_detector.detect_faces(face)\\n  if result :\\n    x1,y1,w,h = result[0]['box']\\n    x2,y2 = np.abs(x1)+w,np.abs(y1)+h\\n    face = face[y1:y2,x1:x2]\\n    return face\\n  else:\\n    return None \\n\\n\\ntrans = Compose([ToTensor(),Normalize(mean=0.4,std=0.2)])\\n\\ndef prepare_data(data):\\n  dataset=[]\\n  count=0\\n  for i in range(len(data)):\\n    face=[]\\n    for j in range(len(data[i])):\\n      f= face_detect(data[i][j])\\n      if f is not None:\\n        if len(f)!=0:\\n          f=cv2.resize(f,(240,240))\\n          f= trans(f)\\n          face.append(f)\\n      else:\\n        count+=1\\n    if len(face)>0:\\n      dataset.append(face)\\n  print(count)\\n  return dataset            \\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN-eO1r_RSGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c99a3116-5904-4e72-f1ed-5e4efe939f16"
      },
      "source": [
        "#loading the res10 ssd model for face detections\n",
        "!gdown --id 1r0R1iLzA3jDCpXqbY94P9rXLTchEs6xo\n",
        "!gdown --id 1Gj7_rdFahB1IOHaJohgJsFVtK8POU_M1\n",
        "prototxtpath ='/content/deploy.prototxt'\n",
        "weightspath ='/content/res10_300x300_ssd_iter_140000.caffemodel'\n",
        "\n",
        "net = cv2.dnn.readNet(weightspath,prototxtpath)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1r0R1iLzA3jDCpXqbY94P9rXLTchEs6xo\n",
            "To: /content/res10_300x300_ssd_iter_140000.caffemodel\n",
            "10.7MB [00:00, 49.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Gj7_rdFahB1IOHaJohgJsFVtK8POU_M1\n",
            "To: /content/deploy.prototxt\n",
            "100% 28.1k/28.1k [00:00<00:00, 25.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbvQVDc1FreL",
        "outputId": "512728b0-a610-4569-b489-e814800bc84c"
      },
      "source": [
        "#function to detect face in selfies \n",
        "\n",
        "def face_detect(net,image):\n",
        "  img = cv2.resize(image,(300,300))\n",
        "  blob = cv2.dnn.blobFromImage(img,1,(300,300),100)\n",
        "  net.setInput(blob)\n",
        "  detections=net.forward()\n",
        "  for i in range(0,detections.shape[2]):\n",
        "    if detections[0,0,i,2]>=0.5:\n",
        "      box = detections[0,0,i,3:7]*np.array([300,300,300,300])\n",
        "      (startx,starty,endx,endy)= box.astype('int')\n",
        "      face = img[starty:endy,startx:endx]\n",
        "      if face is not None: \n",
        "        return face\n",
        "  return None    \n",
        "\n",
        "#transformations to be applied to training data for training model\n",
        "trans = Compose([ToTensor(),Normalize(mean=0.4,std=0.2),Scale((240,240))])\n",
        "\n",
        "#preparing the data by detecting faces and applying transformations\n",
        "def prepare_data(data):\n",
        "  dataset=[]\n",
        "  count=0\n",
        "  for i in range(len(data)):\n",
        "    faces=[]\n",
        "    for j in range(len(data[i])):\n",
        "      f = face_detect(net,data[i][j])\n",
        "      if f is not None:\n",
        "        if len(f)!=0:\n",
        "          f= trans(f)\n",
        "          faces.append(f)\n",
        "      \n",
        "    if len(faces)>0:\n",
        "      dataset.append(faces)\n",
        "    else:\n",
        "      count+=1  \n",
        "  print(count)\n",
        "  return dataset\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:285: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI19qyLLXeA9"
      },
      "source": [
        "selfies_face = prepare_data(selfies)\n",
        "database_faces = prepare_data(database)\n",
        "\n",
        "#loading the already prepared data for training\n",
        "#selfies_face = np.load('drive/MyDrive/selfies_faces.npy',allow_pickle=True)\n",
        "#database_faces = np.load('drive/MyDrive/database_faces.npy',allow_pickle=True)\n",
        "#names = np.load('drive/MyDrive/names.npy',allow_pickle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDwDfaOpe4wF"
      },
      "source": [
        "#saving the pre-processed data for future use\n",
        "np.save('drive/MyDrive/selfies_faces',selfies_face)\n",
        "np.save('drive/MyDrive/database_faces',database_faces)\n",
        "np.save('drive/MyDrive/names',names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkN3zTephuqn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL-94j2YyyQr"
      },
      "source": [
        "# defining the model architecture using pytorch\n",
        "class Face(Module):\n",
        "  def __init__(self,n_channels):\n",
        "    super(Face,self).__init__()\n",
        "    self.conv1 = Conv2d(n_channels,32,(3,3))\n",
        "    kaiming_uniform_(self.conv1.weight,nonlinearity='relu')\n",
        "    self.relu1 = ReLU()\n",
        "    self.pool1 = MaxPool2d((2,2),(2,2))\n",
        "    self.conv2 = Conv2d(32,32,(3,3))\n",
        "    kaiming_uniform_(self.conv2.weight,nonlinearity='relu')\n",
        "    self.relu2 = ReLU()\n",
        "    self.pool2 = MaxPool2d((2,2),(2,2))\n",
        "    self.conv3 = Conv2d(32,64,(3,3))\n",
        "    kaiming_uniform_(self.conv3.weight,nonlinearity='relu')\n",
        "    self.relu3 = ReLU()\n",
        "    self.pool3 = MaxPool2d((2,2),(2,2))\n",
        "    self.conv4 = Conv2d(64,128,(3,3))\n",
        "    kaiming_uniform_(self.conv4.weight,nonlinearity='relu')\n",
        "    self.relu4 = ReLU()\n",
        "    self.pool4 = MaxPool2d((2,2),(2,2))\n",
        "    self.conv5 = Conv2d(128,256,(3,3))\n",
        "    kaiming_uniform_(self.conv5.weight,nonlinearity='relu')\n",
        "    self.relu5 = ReLU()\n",
        "    self.pool5 = MaxPool2d((2,2),(2,2))\n",
        "    self.conv6 = Conv2d(256,512,(3,3))\n",
        "    kaiming_uniform_(self.conv6.weight,nonlinearity='relu')\n",
        "    self.relu6 = ReLU()\n",
        "    self.pool6 = MaxPool2d((2,2),(2,2))\n",
        "    self.layer1 = Linear(1*1*512,128)\n",
        "    kaiming_uniform_(self.layer1.weight,nonlinearity='relu')\n",
        "    self.relu3 = ReLU()\n",
        "    self.layer2 = Linear(128,64)\n",
        "    kaiming_uniform_(self.layer2.weight)\n",
        "    self.relu4 = ReLU()\n",
        "    self.dropout = Dropout(p=0.3)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    x = self.conv1(x)\n",
        "    x=self.relu1(x)\n",
        "    x=self.pool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x=self.relu2(x)\n",
        "    x=self.pool2(x)\n",
        "    x = self.conv3(x)\n",
        "    x=self.relu3(x)\n",
        "    x=self.pool3(x)\n",
        "    x = self.conv4(x)\n",
        "    x=self.relu4(x)\n",
        "    x=self.pool4(x)\n",
        "    x = self.conv5(x)\n",
        "    x=self.relu5(x)\n",
        "    x=self.pool5(x)\n",
        "    x = self.conv6(x)\n",
        "    x=self.relu6(x)\n",
        "    x=self.pool6(x)\n",
        "    x = x.view(-1,1*1*512)\n",
        "    x = self.layer1(x)\n",
        "\n",
        "    x= self.relu3(x)\n",
        "    x= self.layer2(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "model = Face(3)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVud08NFgPFG",
        "outputId": "c5c8f507-ebba-48ec-f525-332da787092c"
      },
      "source": [
        "summary(model,(3,240,240))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 238, 238]             896\n",
            "              ReLU-2         [-1, 32, 238, 238]               0\n",
            "         MaxPool2d-3         [-1, 32, 119, 119]               0\n",
            "            Conv2d-4         [-1, 32, 117, 117]           9,248\n",
            "              ReLU-5         [-1, 32, 117, 117]               0\n",
            "         MaxPool2d-6           [-1, 32, 58, 58]               0\n",
            "            Conv2d-7           [-1, 64, 56, 56]          18,496\n",
            "              ReLU-8           [-1, 64, 56, 56]               0\n",
            "         MaxPool2d-9           [-1, 64, 28, 28]               0\n",
            "           Conv2d-10          [-1, 128, 26, 26]          73,856\n",
            "             ReLU-11          [-1, 128, 26, 26]               0\n",
            "        MaxPool2d-12          [-1, 128, 13, 13]               0\n",
            "           Conv2d-13          [-1, 256, 11, 11]         295,168\n",
            "             ReLU-14          [-1, 256, 11, 11]               0\n",
            "        MaxPool2d-15            [-1, 256, 5, 5]               0\n",
            "           Conv2d-16            [-1, 512, 3, 3]       1,180,160\n",
            "             ReLU-17            [-1, 512, 3, 3]               0\n",
            "        MaxPool2d-18            [-1, 512, 1, 1]               0\n",
            "           Linear-19                  [-1, 128]          65,664\n",
            "             ReLU-20                  [-1, 128]               0\n",
            "           Linear-21                   [-1, 64]           8,256\n",
            "================================================================\n",
            "Total params: 1,651,744\n",
            "Trainable params: 1,651,744\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.66\n",
            "Forward/backward pass size (MB): 44.15\n",
            "Params size (MB): 6.30\n",
            "Estimated Total Size (MB): 51.11\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNQY_qNchPZJ"
      },
      "source": [
        "# training data generator \n",
        "n = 950\n",
        "def generate_dataset(n_triplets=10000):\n",
        "  data=[]\n",
        "  for _ in range(n_triplets):\n",
        "    pos_i = np.random.randint(0,n)\n",
        "    neg_i = np.random.randint(0,n)\n",
        "    while pos_i==neg_i or len(selfies_face[pos_i])<2 :\n",
        "      pos_i = np.random.randint(0,n)\n",
        "    pos_j = np.random.randint(0,len(selfies_face[pos_i]))\n",
        "    anc_j = np.random.randint(0,len(selfies_face[pos_i]))\n",
        "    neg_j = np.random.randint(0,len(selfies_face[neg_i]))\n",
        "    \n",
        "    while anc_j==pos_j:\n",
        "      pos_j = np.random.randint(0,len(selfies_face[pos_i]))\n",
        "    data.append([selfies_face[pos_i][pos_j],selfies_face[pos_i][anc_j],selfies_face[neg_i][neg_j]])\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "id": "Ba4NezcXvdP0",
        "outputId": "0e88a883-a547-41eb-8cad-c8eaea689dfb"
      },
      "source": [
        "#defining the loss function and optimizer\n",
        "triplet_loss = TripletMarginLoss(margin=0.2)\n",
        "optimizer = Adam(model.parameters(),weight_decay=0.0005,lr=0.0001)\n",
        "\n",
        "# training the model on selfies \n",
        "for epoch in range(5):\n",
        "  dataset = DataLoader(generate_dataset(),batch_size=75,shuffle=True)\n",
        "  for id,data in enumerate(dataset):\n",
        "    #print(data[0].shape)\n",
        "    pos_ = model(data[0])\n",
        "    anc_ = model(data[1])\n",
        "    neg_ = model(data[2])\n",
        "    optimizer.zero_grad()\n",
        "    loss = triplet_loss(anc_,pos_,neg_)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print('Epoch :',epoch,' batch : ',id,'    Loss: ',loss.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 0  batch :  0     Loss:  0.07796841859817505\n",
            "Epoch : 0  batch :  1     Loss:  0.062446676194667816\n",
            "Epoch : 0  batch :  2     Loss:  0.07598396390676498\n",
            "Epoch : 0  batch :  3     Loss:  0.0864453911781311\n",
            "Epoch : 0  batch :  4     Loss:  0.06887999176979065\n",
            "Epoch : 0  batch :  5     Loss:  0.09249766170978546\n",
            "Epoch : 0  batch :  6     Loss:  0.08022956550121307\n",
            "Epoch : 0  batch :  7     Loss:  0.07424349337816238\n",
            "Epoch : 0  batch :  8     Loss:  0.084715336561203\n",
            "Epoch : 0  batch :  9     Loss:  0.09653326869010925\n",
            "Epoch : 0  batch :  10     Loss:  0.07942232489585876\n",
            "Epoch : 0  batch :  11     Loss:  0.10603728145360947\n",
            "Epoch : 0  batch :  12     Loss:  0.0818873941898346\n",
            "Epoch : 0  batch :  13     Loss:  0.08114990592002869\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-ea19f534d890>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriplet_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manc_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch :'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' batch : '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'    Loss: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEcn439Wy0eI"
      },
      "source": [
        "#saving the model parameters to google drive\n",
        "torch.save(model.state_dict(),'drive/MyDrive/modelpara')\n",
        "\n",
        "#saving the model to drive\n",
        "torch.save(model,'drive/MyDrive/model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IK2jgJZrXpQA",
        "outputId": "206a145a-dd86-45fb-caac-18357bb727a1"
      },
      "source": [
        "#loading the model from google drive\n",
        "model.load_state_dict(torch.load('drive/MyDrive/modelpara'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdZY5i4E7ggF"
      },
      "source": [
        "#function to calculate cosine similarity between two images\n",
        "def similarity(model,face1,face2):\n",
        "  face1 = torch.unsqueeze(face1,0)\n",
        "  face2 = torch.unsqueeze(face2,0)\n",
        "  face1_f = model(face1)\n",
        "  face2_f = model(face2)\n",
        "  return np.cos(np.linalg.norm((face1_f-face2_f).detach().numpy()))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjYzkWSaozos"
      },
      "source": [
        "# function to give similarity between two images given paths\n",
        "def sim(net,model,path1,path2):\n",
        "  face1=trans(face_detect(net,cv2.imread(path1)))\n",
        "  face2 = trans(face_detect(net,cv2.imread(path2)))\n",
        "  s = similarity(model,face1,face2)\n",
        "  if s>=0.7 :\n",
        "    print('Match with confidence of similarity: %.2f %%'%(s*100))\n",
        "  else:\n",
        "    print('No Match with confidence of similarity : %.2f %%'%(s*100))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAaMt8KF9gad",
        "outputId": "5a29d499-206a-44a9-921f-ca7c8f0e382b"
      },
      "source": [
        "similarity(model,selfies_face[95][0],selfies_face[95][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8850797"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBQJGtKgrt6T",
        "outputId": "291691d2-a760-423e-8b5c-d67bdb94501c"
      },
      "source": [
        "#testing the model on own selfi and passport size photo\n",
        "!gdown --id 123zOxiPxa1LnAdYFMN9iT3KfX-tPsE-x\n",
        "!gdown --id 1Fa0EZ623eTYx5jxrNaMcAGASZLUqMNvi\n",
        "sim(net,model,'/content/IMG_20210224_103333.jpg','/content/gb.jpg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=123zOxiPxa1LnAdYFMN9iT3KfX-tPsE-x\n",
            "To: /content/gb.jpg\n",
            "100% 27.3k/27.3k [00:00<00:00, 42.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Fa0EZ623eTYx5jxrNaMcAGASZLUqMNvi\n",
            "To: /content/IMG_20210224_103333.jpg\n",
            "100% 641k/641k [00:00<00:00, 64.8MB/s]\n",
            "Match with confidence of similarity: 77.83 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAs91QFwr6-7",
        "outputId": "187b7545-dd75-4755-ef13-fedd91377b00"
      },
      "source": [
        "# testing the python script in notebook\n",
        "!python3 match.py -p1 /content/IMG_20210224_103333.jpg -p2 /content/gb.jpg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:285: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n",
            "Match with confidence of similarity: 77.83 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3Q8LSCqD-dY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3yCvWMwFZwW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}